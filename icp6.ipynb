{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "scV_nI5z4IF4",
        "outputId": "f0bf1573-920f-44c5-c63b-d3f6574c6247"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m139/139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 296ms/step - accuracy: 0.4705 - loss: 0.5841 - val_accuracy: 0.5351 - val_loss: -0.2878\n",
            "Epoch 2/5\n",
            "\u001b[1m139/139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 317ms/step - accuracy: 0.6197 - loss: -1.2020 - val_accuracy: 0.5527 - val_loss: -0.9492\n",
            "Epoch 3/5\n",
            "\u001b[1m139/139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 298ms/step - accuracy: 0.6217 - loss: -3.0504 - val_accuracy: 0.5851 - val_loss: -1.9809\n",
            "Epoch 4/5\n",
            "\u001b[1m139/139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 295ms/step - accuracy: 0.6336 - loss: -4.0125 - val_accuracy: 0.5887 - val_loss: -2.4593\n",
            "Epoch 5/5\n",
            "\u001b[1m139/139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 293ms/step - accuracy: 0.6435 - loss: -7.3315 - val_accuracy: 0.5523 - val_loss: -2.1986\n",
            "Epoch 1/5\n",
            "\u001b[1m139/139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 292ms/step - accuracy: 0.3492 - loss: 0.6407 - val_accuracy: 0.4396 - val_loss: 0.3723\n",
            "Epoch 2/5\n",
            "\u001b[1m139/139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 298ms/step - accuracy: 0.5639 - loss: 0.0082 - val_accuracy: 0.5455 - val_loss: -0.4702\n",
            "Epoch 3/5\n",
            "\u001b[1m139/139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 289ms/step - accuracy: 0.6057 - loss: -1.2092 - val_accuracy: 0.5369 - val_loss: -0.4444\n",
            "Epoch 4/5\n",
            "\u001b[1m139/139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 310ms/step - accuracy: 0.6078 - loss: -1.7862 - val_accuracy: 0.5973 - val_loss: -1.6177\n",
            "Epoch 5/5\n",
            "\u001b[1m139/139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 295ms/step - accuracy: 0.6541 - loss: -3.2302 - val_accuracy: 0.5838 - val_loss: -1.9068\n",
            "Best accuracy: 0.6093693971633911 with optimizer: rmsprop\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 220ms/step\n",
            "Prediction: 0.6635391116142273\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('Sentiment.csv')\n",
        "\n",
        "# Preprocessing\n",
        "X = data['text'].values\n",
        "y = data['sentiment'].values\n",
        "\n",
        "# Encode labels to numerical values\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(y)\n",
        "\n",
        "tokenizer = Tokenizer(num_words=5000)\n",
        "tokenizer.fit_on_texts(X)\n",
        "X = tokenizer.texts_to_sequences(X)\n",
        "X = pad_sequences(X, maxlen=100)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Build LSTM model\n",
        "def create_model(optimizer='adam'):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim=5000, output_dim=128, input_length=100))\n",
        "    model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Hyperparameter tuning\n",
        "optimizers = ['adam', 'rmsprop']\n",
        "best_accuracy = 0\n",
        "best_optimizer = ''\n",
        "\n",
        "for optimizer in optimizers:\n",
        "    model = create_model(optimizer)\n",
        "    checkpoint = ModelCheckpoint('model.keras', monitor='val_loss', save_best_only=True)\n",
        "    model.fit(X_train, y_train, validation_split=0.2, epochs=5, batch_size=64, callbacks=[checkpoint])\n",
        "    accuracy = model.evaluate(X_test, y_test, verbose=0)[1]\n",
        "    if accuracy > best_accuracy:\n",
        "        best_accuracy = accuracy\n",
        "        best_optimizer = optimizer\n",
        "\n",
        "print(f\"Best accuracy: {best_accuracy} with optimizer: {best_optimizer}\")\n",
        "\n",
        "# Load the saved model and predict on new text data\n",
        "saved_model = load_model('model.keras')\n",
        "new_text = [\"A lot of good things are happening. We are respected again throughout the world, and that's a great thing.@realDonaldTrump\"]\n",
        "new_text_seq = tokenizer.texts_to_sequences(new_text)\n",
        "new_text_pad = pad_sequences(new_text_seq, maxlen=100)\n",
        "prediction = saved_model.predict(new_text_pad)\n",
        "print(f\"Prediction: {prediction[0][0]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GwAdMDopoedo"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}